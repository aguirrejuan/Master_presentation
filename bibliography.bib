%FCN  
@article{van2014scikit,
  title={scikit-image: image processing in Python},
  author={Van der Walt, Stefan and Sch{\"o}nberger, Johannes L and Nunez-Iglesias, Juan and Boulogne, Fran{\c{c}}ois and Warner, Joshua D and Yager, Neil and Gouillart, Emmanuelle and Yu, Tony},
  journal={PeerJ},
  volume={2},
  pages={e453},
  year={2014},
  publisher={PeerJ Inc.}
}

@inproceedings{nguyen2023embryosformer,
  title={EmbryosFormer: Deformable Transformer and Collaborative Encoding-Decoding for Embryos Stage Development Classification},
  author={Nguyen, Tien-Phat and Pham, Trong-Thang and Nguyen, Tri and Le, Hieu and Nguyen, Dung and Lam, Hau and Nguyen, Phong and Fowler, Jennifer and Tran, Minh-Triet and Le, Ngan},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={1981--1990},
  year={2023}
}

@article{peng2021random,
  title={Random feature attention},
  author={Peng, Hao and Pappas, Nikolaos and Yogatama, Dani and Schwartz, Roy and Smith, Noah A and Kong, Lingpeng},
  journal={arXiv preprint arXiv:2103.02143},
  year={2021}
}

@article{jimenez2021random,
  title={Random fourier features-based deep learning improvement with class activation interpretability for nerve structure segmentation},
  author={Jimenez-Casta{\~n}o, Cristian Alfonso and {\'A}lvarez-Meza, Andr{\'e}s Marino and Aguirre-Ospina, Oscar David and C{\'a}rdenas-Pe{\~n}a, David Augusto and Orozco-Guti{\'e}rrez, {\'A}lvaro Angel},
  journal={Sensors},
  volume={21},
  number={22},
  pages={7741},
  year={2021},
  publisher={MDPI}
}


@article{zhang2021dive,
  title={Dive into deep learning},
  author={Zhang, Aston and Lipton, Zachary C and Li, Mu and Smola, Alexander J},
  journal={arXiv preprint arXiv:2106.11342},
  year={2021}
}

@article{LongSD14,
  author    = {Jonathan Long and
               Evan Shelhamer and
               Trevor Darrell},
  title     = {Fully Convolutional Networks for Semantic Segmentation},
  journal   = {CoRR},
  volume    = {abs/1411.4038},
  year      = {2014},
  url       = {http://arxiv.org/abs/1411.4038},
  eprinttype = {arXiv},
  eprint    = {1411.4038},
  timestamp = {Mon, 13 Aug 2018 16:48:17 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/LongSD14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

%UNET
@article{RonnebergerFB15,
  author    = {Olaf Ronneberger and
               Philipp Fischer and
               Thomas Brox},
  title     = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
  journal   = {CoRR},
  volume    = {abs/1505.04597},
  year      = {2015},
  url       = {http://arxiv.org/abs/1505.04597},
  eprinttype = {arXiv},
  eprint    = {1505.04597},
  timestamp = {Mon, 13 Aug 2018 16:46:52 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/RonnebergerFB15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

%RESUNET
@article{DiakogiannisCC19,
  author    = {Foivos I. Diakogiannis and
               Fran{\c{c}}ois Waldner and
               Peter Caccetta and
               Chen Wu},
  title     = {ResUNet-a: a deep learning framework for semantic segmentation of
               remotely sensed data},
  journal   = {CoRR},
  volume    = {abs/1904.00592},
  year      = {2019},
  url       = {http://arxiv.org/abs/1904.00592},
  eprinttype = {arXiv},
  eprint    = {1904.00592},
  timestamp = {Wed, 24 Apr 2019 12:21:25 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1904-00592.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{resunet17,
author="Anas, Emran Mohammad Abu
and Nouranian, Saman
and Mahdavi, S. Sara
and Spadinger, Ingrid
and Morris, William J.
and Salcudean, Septimu E.
and Mousavi, Parvin
and Abolmaesumi, Purang",
editor="Descoteaux, Maxime
and Maier-Hein, Lena
and Franz, Alfred
and Jannin, Pierre
and Collins, D. Louis
and Duchesne, Simon",
title="Clinical Target-Volume Delineation in Prostate Brachytherapy Using Residual Neural Networks",
booktitle="Medical Image Computing and Computer Assisted Intervention − MICCAI 2017",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="365--373",
abstract="Low dose-rate prostate brachytherapy is commonly used to treat early stage prostate cancer. This intervention involves implanting radioactive seeds inside a volume containing the prostate. Planning the intervention requires obtaining a series of ultrasound images from the prostate. This is followed by delineation of a clinical target volume, which mostly traces the prostate boundary in the ultrasound data, but can be modified based on institution-specific clinical guidelines. Here, we aim to automate the delineation of clinical target volume by using a new deep learning network based on residual neural nets and dilated convolution at deeper layers. In addition, we propose to include an exponential weight map in the optimization to improve local prediction. We train the network on 4,284 expert-labeled transrectal ultrasound images and test it on an independent set of 1,081 ultrasound images. With respect to the gold-standard delineation, we achieve a mean Dice similarity coefficient of 94{\%}, a mean surface distance error of 1.05 mm and a mean Hausdorff distance error of 3.0 mm. The obtained results are statistically significantly better than two previous state-of-the-art techniques.",
isbn="978-3-319-66179-7"
}


@misc{VGG,
  doi = {10.48550/ARXIV.1409.1556},
  
  url = {https://arxiv.org/abs/1409.1556},
  
  author = {Simonyan, Karen and Zisserman, Andrew},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{ZhouKLOT15,
  author    = {Bolei Zhou and
               Aditya Khosla and
               {\`{A}}gata Lapedriza and
               Aude Oliva and
               Antonio Torralba},
  title     = {Learning Deep Features for Discriminative Localization},
  journal   = {CoRR},
  volume    = {abs/1512.04150},
  year      = {2015},
  url       = {http://arxiv.org/abs/1512.04150},
  eprinttype = {arXiv},
  eprint    = {1512.04150},
  timestamp = {Mon, 13 Aug 2018 16:47:46 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ZhouKLOT15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Jiang2021,
   abstract = {The class activation maps are generated from the final convolutional layer of CNN. They can highlight discriminative object regions for the class of interest. These discovered object regions have been widely used for weakly-supervised tasks. However, due to the small spatial resolution of the final convolutional layer, such class activation maps often locate coarse regions of the target objects, limiting the performance of weakly-supervised tasks that need pixel-accurate object locations. Thus, we aim to generate more fine-grained object localization information from the class activation maps to locate the target objects more accurately. In this paper, by rethinking the relationships between the feature maps and their corresponding gradients, we propose a simple yet effective method, called LayerCAM. It can produce reliable class activation maps for different layers of CNN. This property enables us to collect object localization information from coarse (rough spatial localization) to fine (precise fine-grained details) levels. We further integrate them into a high-quality class activation map, where the object-related pixels can be better highlighted. To evaluate the quality of the class activation maps produced by LayerCAM, we apply them to weakly-supervised object localization and semantic segmentation. Experiments demonstrate that the class activation maps generated by our method are more effective and reliable than those by the existing attention methods. The code will be made publicly available.},
   author = {Peng Tao Jiang and Chang Bin Zhang and Qibin Hou and Ming Ming Cheng and Yunchao Wei},
   doi = {10.1109/TIP.2021.3089943},
   issn = {19410042},
   journal = {IEEE Transactions on Image Processing},
   keywords = {Weakly-supervised object localization,class activation maps},
   pages = {5875-5888},
   pmid = {34156941},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {LayerCAM: Exploring hierarchical class activation maps for localization},
   volume = {30},
   year = {2021},
}

@article{segCam,
   abstract = {Convolutional neural networks have become state-of-the-art in a wide range of image recognition tasks. The interpretation of their predictions, however, is an active area of research. Whereas various interpretation methods have been suggested for image classification, the interpretation of image segmen-tation still remains largely unexplored. To that end, we propose SEG-GRAD-CAM, a gradient-based method for interpreting semantic segmentation. Our method is an extension of the widely-used Grad-CAM method, applied locally to produce heatmaps showing the relevance of individual pixels for semantic segmentation.},
   author = {Kira Vinogradova and Alexandr Dibrov and Gene Myers},
   title = {Towards Interpretable Semantic Segmentation via Gradient-weighted Class Activation Mapping},
}


@InProceedings{DeepsemanticUnal,
author="Mejia-Zuluaga, Rafael
and Aguirre-Arango, Juan Carlos
and Collazos-Huertas, Diego
and Daza-Castillo, Jessica
and Valencia-Marulanda, N{\'e}stor
and Calder{\'o}n-Marulanda, Mauricio
and Aguirre-Ospina, {\'O}scar
and Alvarez-Meza, Andr{\'e}s
and Castellanos-Dominguez, Germ{\'a}n",
editor="Bicharra Garcia, Ana Cristina
and Ferro, Mariza
and Rodr{\'i}guez Rib{\'o}n, Julio Cesar",
title="Deep Learning Semantic Segmentation of Feet Using Infrared Thermal Images",
booktitle="Advances in Artificial Intelligence -- IBERAMIA 2022",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="342--352",
abstract="Regional neuraxial analgesia is a safe method for pain relief during labor, but its effectiveness must be assessed carefully. As a non-invasive technique, thermal imaging is gaining increasing acclaim as an objective way to quantify blood flow redistribution-related warm modifications. Hence, thermal measurements are acquired under controlled conditions at different timestamps to determine the anesthesia depth by characterizing earlier thermal changes. However, the procedures during labor are limited by two main factors: a relatively small sample size is possible and thermal images cannot be acquired with both feet in the same position. This work implements an automatic semantic segmentation approach using five state-of-the-art deep learning architectures and an artifact removal algorithm based on morphological operators to deal with this problem. The obtained results are evaluated on two databases (acquired at the Universidad Nacional de Colombia sede Manizales and the SES Hospital Universitario de Caldas): controlled and uncontrolled environments for thermal data acquisition. Obtained results indicate that U-Mobilenetv2 approach outperforms the rest of the compared models.",
isbn="978-3-031-22419-5"
}



@InProceedings{parkhi12a,
  author       = "Parkhi, O. M. and Vedaldi, A. and Zisserman, A. and Jawahar, C.~V.",
  title        = "Cats and Dogs",
  booktitle    = "IEEE Conference on Computer Vision and Pattern Recognition",
  year         = "2012",
}

@inproceedings{Rahimi2009,
   abstract = {To accelerate the training of kernel machines, we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods. The features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user specified shiftinvariant kernel. We explore two sets of random features, provide convergence bounds on their ability to approximate various radial basis kernels, and show that in large-scale classification and regression tasks linear machine learning algorithms applied to these features outperform state-of-the-art large-scale kernel machines.},
   author = {Ali Rahimi and Benjamin Recht},
   journal = {Advances in Neural Information Processing Systems 20 - Proceedings of the 2007 Conference},
   title = {Random features for large-scale kernel machines},
   year = {2009},
}

@book{bochner,
title = {Fourier analysis on groups},
series = {Interscience tracts in pure and applied mathematics ;},
author = {Rudin, Walter 1921-2010},
address = {New York, New York},
publisher = {Interscience},
year = {1976},
pages = {19},
}

@InProceedings{gaussinaTratability,
author="{\'A}lvarez-Meza, A. M.
and C{\'a}rdenas-Pe{\~{n}}a, D.
and Castellanos-Dominguez, Germ{\'a}n",
editor="Bayro-Corrochano, Eduardo
and Hancock, Edwin",
title="Unsupervised Kernel Function Building Using Maximization of Information Potential Variability",
booktitle="Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications",
year="2014",
publisher="Springer International Publishing",
address="Cham",
pages="335--342",
abstract="We propose a kernel function estimation strategy to support machine learning tasks by analyzing the input samples using Renyi's Information Metrics. Specifically, we aim to identify a Reproducing Kernel Hilbert Space spanning the most widely the information force among data points by the maximization of the information potential variability of Parzen-based pdf estimation. So, a Gaussian kernel bandwidth updating rule is obtained as a function of the forces induced by a given dataset. Our proposal is tested on synthetic and real-world datasets related to clustering and classification tasks. Obtained results show that presented approach allows to compute RKHS's favoring data groups separability, attaining suitable learning performances in comparison with state of the art algorithms.",
isbn="978-3-319-12568-8"
}


@article{geometricDeepLearning,
  author    = {Michael M. Bronstein and
               Joan Bruna and
               Taco Cohen and
               Petar Velickovic},
  title     = {Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges},
  journal   = {CoRR},
  volume    = {abs/2104.13478},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.13478},
  eprinttype = {arXiv},
  eprint    = {2104.13478},
  timestamp = {Tue, 04 May 2021 15:12:43 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-13478.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{Bruins2017ThermographicST,
  title={Thermographic skin temperature measurement compared with cold sensation in predicting the efficacy and distribution of epidural anesthesia},
  author={Arnoud A. Bruins and Kay R J Kistemaker and A. Boom and J. Klaessens and R. Verdaasdonk and C. Boer},
  journal={Journal of Clinical Monitoring and Computing},
  year={2018},
  volume={32},
  pages={335 - 341}
}


@article{2143r23,
author = {Haren, F and Kadic, Lajla and Driessen, Jacques},
year = {2013},
month = {08},
pages = {},
title = {Skin temperature measured by infrared thermography after ultrasound-guided blockade of the sciatic nerve},
volume = {57},
journal = {Acta anaesthesiologica Scandinavica},
doi = {10.1111/aas.12170}
}


@article{10.1093/bja/52.6.589,
    author = {Brown, D. T. and Wildsmith, J. A. W. and Covino, B. G. and Scott, D. B.},
    title = "{Effect of Baricity on Spinal Anaesthesia with Amethocaine}",
    journal = {BJA: British Journal of Anaesthesia},
    volume = {52},
    number = {6},
    pages = {589-596},
    year = {1980},
    month = {06},
    abstract = "{Amethocaine 1\% solution was mixed with equal volumes of water, 0.9\% saline or 10\% dextrose to prepare respectively, hypobaric, isobaric and hyperbaric solutions which were compared for intradural spinal anaesthesia in 60 patients. Thirty patients received 10 rag and 30 patients received 15 mg of amethocaine. Injections were made with the patients in the lateral recumbent position and the operating table was horizontal during and after injection. Equal numbers of patients and equal numbers of males and females received hypobaric, isobaric and hyperbaric solutions. The mean spread of analgesia after the hyperbaric solution was five dermatomes greater than after the other two solutions, but the extent of analgesia was not significantly different whether amethocaine 10 mg or 15 mg was injected. The mean duration of analgesia after the hyperbaric solution was 285 min compared with 332 min and 360 min after the isobaric and hypobaric solutions respectively. The mean duration of analgesia after amethocaine 15 mg was significantly greater than after 10 mg.}",
    issn = {0007-0912},
    doi = {10.1093/bja/52.6.589},
    url = {https://doi.org/10.1093/bja/52.6.589},
    eprint = {https://academic.oup.com/bja/article-pdf/52/6/589/822181/52-6-589.pdf},
}


@article{9a56ceb48ddd41fb9fb1073a5781386f,
title = "Thermographic skin temperature measurement compared with cold sensation in predicting the efficacy and distribution of epidural anesthesia",
abstract = "Due to the high rates of epidural failure (3–32\%), novel techniques are required to objectively assess the successfulness of an epidural block. In this study we therefore investigated whether thermographic temperature measurements have a higher predictive value for a successful epidural block when compared to the cold sensation test as gold standard. Epidural anesthesia was induced in 61 patients undergoing elective abdominal, thoracic or orthopedic surgery. A thermographic picture was recorded at 5, 10 and 15 min following epidural anesthesia induction. After 15 min a cold sensation test was performed. Epidural anesthesia is associated with a decrease in skin temperature. Thermography predicts a successful epidural block with a sensitivity of 54\% and a PPV of 92\% and a specificity of 67\% and a NPV of 17\%. The cold sensation test shows a higher sensitivity and PPV than thermography (97 and 93\%), but a lower specificity and NPV than thermography (25 and 50\%). Thermographic temperature measurements can be used as an additional and objective method for the assessment of the effectiveness of an epidural block next to the cold sensation test, but have a low sensitivity and negative predictive value. The local decrease in temperature as observed in our study during epidural anesthesia is mainly attributed to a core-to-peripheral redistribution of body heat and vasodilation.",
keywords = "Cold sensation test, Epidural anesthesia, Postoperative pain, Thermography",
author = "Bruins, {Arnoud A.} and Kistemaker, {Kay R.J.} and Annemieke Boom and Klaessens, {John H.G.M.} and Verdaasdonk, {Rudolf M.} and Christa Boer",
year = "2018",
month = apr,
doi = "10.1007/s10877-017-0026-y",
language = "English",
volume = "32",
pages = "335--341",
journal = "Journal of clinical monitoring and computing",
issn = "1387-1307",
publisher = "Springer",
number = "2",
}

@article{alzubaidi2020towards,
  title={Towards a better understanding of transfer learning for medical imaging: a case study},
  author={Alzubaidi, Laith and Fadhel, Mohammed A and Al-Shamma, Omran and Zhang, Jinglan and Santamar{\'\i}a, J and Duan, Ye and R. Oleiwi, Sameer},
  journal={Applied Sciences},
  volume={10},
  number={13},
  pages={4523},
  year={2020},
  publisher={MDPI}
}


@misc{nakkiran2019deep,
      title={Deep Double Descent: Where Bigger Models and More Data Hurt}, 
      author={Preetum Nakkiran and Gal Kaplun and Yamini Bansal and Tristan Yang and Boaz Barak and Ilya Sutskever},
      year={2019},
      eprint={1912.02292},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{heckel2020early,
      title={Early Stopping in Deep Networks: Double Descent and How to Eliminate it}, 
      author={Reinhard Heckel and Fatih Furkan Yilmaz},
      year={2020},
      eprint={2007.10099},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{haji2021comparison,
  title={Comparison of optimization techniques based on gradient descent algorithm: A review},
  author={Haji, Saad Hikmat and Abdulazeez, Adnan Mohsin},
  journal={PalArch's Journal of Archaeology of Egypt/Egyptology},
  volume={18},
  number={4},
  pages={2715--2743},
  year={2021}
}

@misc{karimi2022critical,
      title={Critical Assessment of Transfer Learning for Medical Image Segmentation with Fully Convolutional Neural Networks}, 
      author={Davood Karimi and Simon K. Warfield and Ali Gholipour},
      year={2022},
      eprint={2006.00356},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@ARTICLE{9557808,
  author={Guan, Hao and Liu, Mingxia},
  journal={IEEE Transactions on Biomedical Engineering}, 
  title={Domain Adaptation for Medical Image Analysis: A Survey}, 
  year={2022},
  volume={69},
  number={3},
  pages={1173-1185},
  doi={10.1109/TBME.2021.3117407}}


@misc{ruan2022vision,
      title={Vision Transformers: State of the Art and Research Challenges}, 
      author={Bo-Kai Ruan and Hong-Han Shuai and Wen-Huang Cheng},
      year={2022},
      eprint={2207.03041},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article {PMID:16551932,
	Title = {Skin temperature during regional anesthesia of the lower extremity},
	Author = {Stevens, Markus F and Werdehausen, Robert and Hermanns, Henning and Lipfert, Peter},
	DOI = {10.1213/01.ane.0000198627.16144.77},
	Number = {4},
	Volume = {102},
	Month = {April},
	Year = {2006},
	Journal = {Anesthesia and analgesia},
	ISSN = {0003-2999},
	Pages = {1247—1251},
	Abstract = {Increase in skin temperature (Ts) occurs early during neuraxial blocks. However, the reliability of Ts to predict successful peripheral block is unknown. Therefore, we investigated whether an increase in Ts more than 1 degrees C precedes or follows an impairment of sensation after combined femoral and sciatic nerve block as well as after epidural anesthesia. In this prospective, nonrandomized study we determined Ts changes in 33 patients undergoing knee or foot surgery under femoral and sciatic nerve block and 10 patients undergoing epidural anesthesia. Perception and motor function were assessed every 5 min. An increase in Ts (&gt; or =1 degrees C) at the foot occurred later after sciatic nerve block than after epidural anesthesia (10.3 +/- 2.8 versus 5.0 min; P &lt; 0.01). Alterations of Ts at skin innervated by the femoral nerve were &lt;1 degrees C. Ts increase preceded sensory block after sciatic nerve block in 6.6\% of patients but indicated a successful block (sensitivity, specificity, and accuracy = 100\%). We conclude that an increase of Ts is a reliable, but late, sign of successful sciatic nerve block. Therefore it is of limited clinical value. Ts changes after femoral nerve block are negligible and late.},
	URL = {https://doi.org/10.1213/01.ane.0000198627.16144.77},
}


@article{WERDEHAUSEN200773,
title = {Uniform Distribution of Skin-Temperature Increase After Different Regional-Anesthesia Techniques of the Lower Extremity},
journal = {Regional Anesthesia and Pain Medicine},
volume = {32},
number = {1},
pages = {73-78},
year = {2007},
issn = {1098-7339},
doi = {https://doi.org/10.1016/j.rapm.2006.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S1098733906005281},
author = {Robert Werdehausen and Sebastian Braun and Henning Hermanns and Rainer Freynhagen and Peter Lipfert and Markus F. Stevens},
keywords = {Epidural anesthesia, Spinal anesthesia, Nerve block, Skin temperature, Thermographic imaging, Sympathetic block},
abstract = {Background and Objectives
Skin-temperature increase is a reliable but late indicator of success during regional-anesthesia techniques. The goal of this study is to determine the distribution of skin-temperature changes during different regional techniques. Does skin temperature increase in the whole area innervated by the blocked neural structures or only in certain regions within this area with the capability to react preferentially to sympathetic block (i.e., vessel-rich skin)? Although onset time may vary between different regional-anesthetic techniques, we hypothesized that the distribution of skin warming is equal.
Methods
Skin temperature was assessed continuously by infrared thermography in 24 patients who received either combined femoral-nerve and sciatic-nerve block, epidural anesthesia, or spinal anesthesia.
Results
Apart from differences in time of onset, no differential spatial distribution of skin-temperature changes could be detected. The earliest and greatest rise of skin temperature occurred at the great toe (10.6°C ± 0.4°C), became smaller proximally, and was negligible above the ankles, irrespective of the type and extent of block. Videothermography revealed that cold blood flows through subcutaneous veins immediately after onset of sympathetic block and initially decreases skin temperature (0.6°C ± 0.3°C) during onset of spinal anesthesia.
Conclusion
Irrespective of the applied regional-anesthetic technique, skin-temperature changes are more pronounced distally. Thermography prevents false measurements of skin temperature above subcutaneous veins and displays flow of cold blood as the mechanism of initial skin-temperature drop after regional anesthesia. Measurements of skin-temperature increase cannot be used to evaluate the extent of analgesia or sympathetic block.}
}


@article{zhang2022real,
  title={Real-time segmentation method of billet infrared image based on multi-scale feature fusion},
  author={Zhang, Lixin and Nan, Qingrong and Bian, Shengqin and Liu, Tao and Xu, Zhengguang},
  journal={Scientific Reports},
  volume={12},
  number={1},
  pages={6879},
  year={2022},
  publisher={Nature Publishing Group UK London}
}


@misc{https://doi.org/10.48550/arxiv.2205.13278,
  doi = {10.48550/ARXIV.2205.13278},
  
  url = {https://arxiv.org/abs/2205.13278},
  
  author = {Kütük, Zülfiye and Algan, Görkem},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Semantic Segmentation for Thermal Images: A Comparative Survey},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@article{Wang2022,
   abstract = {The standard kernel method is computationally expensive because it needs to store and compute the inverse of the Gram matrix. Furthermore, the classification accuracy of the single kernel method is not effective or efficient as the method's ability to extract features. The random Fourier feature method establishes a connection between the kernel method and deep learning to solve the above problems. In this paper, we propose a novel slim deep random Fourier feature network named Slim-RFFNet, which introduces convolution into kernel learning. We use the hierarchical strategy and skip connection to construct a deep network structure and lighten the model by using quantization. Experiments conducted on classification benchmarks MNIST and CIFAR10 demonstrate that the proposed Slim-RFFNet significantly outperforms current state-of-the-art deep kernel learning methods. Our algorithm also achieves a trade-off between accuracy and latency. The proposed network can be applied to resource-constrained embedded AI devices. The experimental results on the edge computing system show that our algorithm has a small memory footprint and fast inference speed on small edge devices, and thus meets the requirements for practical applications.},
   author = {Tingting Wang and Bo Dong and Kaichun Zhang and Junbao Li and Lei Xu},
   doi = {10.1016/j.knosys.2021.107878},
   issn = {09507051},
   journal = {Knowledge-Based Systems},
   keywords = {Classification,Convolution,Deep kernel learning,Quantization,Random Fourier features},
   month = {2},
   pages = {107878},
   publisher = {Elsevier B.V.},
   title = {Slim-RFFNet: Slim deep convolution random Fourier feature network for image classification},
   volume = {237},
   year = {2022},
}

@article{Wang2021,
   abstract = {Tracking the instruments in a surgical scene is an essential task in minimally invasive surgery. However, due to the unpredictability of scenes, automatically segmenting the instruments is very challenging. In this paper, a novel method named parallel inception network (PaI-Net) is proposed, in which an attention parallel module (APM) and an output fusion module (OFM) are integrated with U-Net to improve the segmentation ability. Specially, APM utilizes multi-scale convolution kernels and global average pooling operations to extract semantic information and global context information of different scales, while OFM combines the feature maps of the decoder part to aggregate the abundant boundary information of shallow layers and the rich semantic information of deep layers together, which achieve a significant improvement in generating segmentation masks. Finally, the evaluation of proposed method on robotic instruments segmentation task from Medical Image Computing and Computer Assisted Intervention Society (MICCAI) and retinal image segmentation task from International Symposium on Biomedical Imaging (ISBI) show that our model has achieved advanced performance on multi-scale semantic segmentation and is superior to the current state-of-the-art models.},
   author = {Xiaoyan Wang and Luyao Wang and Xingyu Zhong and Cong Bai and Xiaojie Huang and Ruiyi Zhao and Ming Xia},
   doi = {10.1049/IPR2.12283},
   issn = {17519659},
   issue = {12},
   journal = {IET Image Processing},
   month = {10},
   pages = {2959-2969},
   publisher = {John Wiley and Sons Inc},
   title = {PaI-Net: A modified U-Net of reducing semantic gap for surgical instrument segmentation},
   volume = {15},
   url = {https://www.researchgate.net/publication/352976003_PaI-Net_A_modified_U-Net_of_reducing_semantic_gap_for_surgical_instrument_segmentation},
   year = {2021},
}


@article{Zhou2018,
   abstract = {In this paper, we present UNet++, a new, more powerful architecture for medical image segmentation. Our architecture is essentially a deeply-supervised encoder-decoder network where the encoder and decoder sub-networks are connected through a series of nested, dense skip pathways. The re-designed skip pathways aim at reducing the semantic gap between the feature maps of the encoder and decoder sub-networks. We argue that the optimizer would deal with an easier learning task when the feature maps from the decoder and encoder networks are semantically similar. We have evaluated UNet++ in comparison with U-Net and wide U-Net architectures across multiple medical image segmentation tasks: nodule segmentation in the low-dose CT scans of chest, nuclei segmentation in the microscopy images, liver segmentation in abdominal CT scans, and polyp segmentation in colonoscopy videos. Our experiments demonstrate that UNet++ with deep supervision achieves an average IoU gain of 3.9 and 3.4 points over U-Net and wide U-Net, respectively.},
   author = {Zongwei Zhou and Md Mahfuzur Rahman Siddiquee and Nima Tajbakhsh and Jianming Liang},
   doi = {10.1007/978-3-030-00889-5_1},
   isbn = {9783030008888},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   month = {9},
   pages = {3-11},
   publisher = {Springer, Cham},
   title = {UNet++: A Nested U-Net Architecture for Medical Image Segmentation},
   volume = {11045 LNCS},
   url = {https://link.springer.com/chapter/10.1007/978-3-030-00889-5_1},
   year = {2018},
}


@article{Ibtehaz2020,
   abstract = {In recent years Deep Learning has brought about a breakthrough in Medical Image Segmentation. In this regard, U-Net has been the most popular architecture in the medical imaging community. Despite outstanding overall performance in segmenting multimodal medical images, through extensive experimentations on some challenging datasets, we demonstrate that the classical U-Net architecture seems to be lacking in certain aspects. Therefore, we propose some modifications to improve upon the already state-of-the-art U-Net model. Following these modifications, we develop a novel architecture, MultiResUNet, as the potential successor to the U-Net architecture. We have tested and compared MultiResUNet with the classical U-Net on a vast repertoire of multimodal medical images. Although only slight improvements in the cases of ideal images are noticed, remarkable gains in performance have been attained for the challenging ones. We have evaluated our model on five different datasets, each with their own unique challenges, and have obtained a relative improvement in performance of 10.15\%, 5.07\%, 2.63\%, 1.41\%, and 0.62\% respectively. We have also discussed and highlighted some qualitatively superior aspects of MultiResUNet over classical U-Net that are not really reflected in the quantitative measures.},
   author = {Nabil Ibtehaz and M. Sohel Rahman},
   doi = {10.1016/J.NEUNET.2019.08.025},
   issn = {18792782},
   journal = {Neural Networks},
   keywords = {Convolutional neural networks,Medical imaging,Semantic segmentation,U-Net},
   month = {1},
   pages = {74-87},
   pmid = {31536901},
   publisher = {Elsevier Ltd},
   title = {MultiResUNet: Rethinking the U-Net architecture for multimodal biomedical image segmentation},
   volume = {121},
   year = {2020},
}


@article{feetreview,
   abstract = {Thermography enables non-invasive, accessible, and easily repeated foot temperature measurements for diabetic patients, promoting early detection and regular monitoring protocols, that limit the incidence of disabling conditions associated with diabetic foot disorders. The establishment of this application into standard diabetic care protocols requires to overcome technical issues, particularly the foot sole segmentation. In this work we implemented and evaluated several segmentation approaches which include conventional and Deep Learning methods. Multimodal images, constituted by registered visual-light, infrared and depth images, were acquired for 37 healthy subjects. The segmentation methods explored were based on both visual-light as well as infrared images, and optimization was achieved using the spatial information provided by the depth images. Furthermore, a ground truth was established from the manual segmentation performed by two independent researchers. Overall, the performance level of all the implemented approaches was satisfactory. Although the best performance, in terms of spatial overlap, accuracy, and precision, was found for the Skin and U-Net approaches optimized by the spatial information. However, the robustness of the U-Net approach is preferred.},
   author = {Natalia Arteaga-Marrero and Abián Hernández and Enrique Villa and Sara González-Pérez and Carlos Luque and Juan Ruiz-Alzola},
   doi = {10.3390/S21030934},
   issn = {1424-8220},
   issue = {3},
   journal = {Sensors 2021, Vol. 21, Page 934},
   keywords = {diabetic foot (D017719),diabetic neuropathy (D003929),segmentation,supervised and unsupervised algorithms,thermography (D013817)},
   month = {1},
   pages = {934},
   pmid = {33573296},
   publisher = {Multidisciplinary Digital Publishing Institute},
   title = {Segmentation Approaches for Diabetic Foot Disorders},
   volume = {21},
   url = {https://www.mdpi.com/1424-8220/21/3/934/htm https://www.mdpi.com/1424-8220/21/3/934},
   year = {2021},
}





%acquisition 
@article {PMID:16428569,
	Title = {Thermographic temperature measurement compared with pinprick and cold sensation in predicting the effectiveness of regional blocks},
	Author = {Galvin, Eilish M and Niehof, Sjoerd and Medina, Hector J and Zijlstra, Freek J and van Bommel, Jasper and Klein, Jan and Verbrugge, Serge J C},
	DOI = {10.1213/01.ane.0000189556.49429.16},
	Number = {2},
	Volume = {102},
	Month = {February},
	Year = {2006},
	Journal = {Anesthesia and analgesia},
	ISSN = {0003-2999},
	Pages = {598—604},
	Abstract = {We designed this study to evaluate the usefulness of thermographic temperature measurement with an infrared camera, compared with patient response to cold and pinprick, as a means of assessing the success or failure of axillary blockades. Axillary blocks were performed on 25 patients undergoing surgery on the hand or forearm using a nerve stimulator technique with mepivacaine 1.5\%. Pinprick and cold sensation were assessed on the operative site at 5-min intervals for 30 min. A thermographic image of the operative limb was recorded at similar time intervals. Thermographic images of the unblocked limb were taken before block placement and at 30 min. Temperature values at the operative site and unblocked limb were calculated from the thermographic images. Results revealed that thermography had higher combined values for sensitivity, specificity, and positive and negative predictive values than both cold and pinprick at all time intervals, with statistically significant differences at 15 min (thermography versus cold, P = 0.006; thermography versus pinprick, P = 0.026) and 30 min (thermography versus cold, P = 0.038; thermography versus pinprick, P = 0.040). For thermography as a method of block assessment, an optimal time of 15 min after mepivacaine local anesthetic injection gives the highest combined values for predicting a successful block (P = 0.004). We conclude that thermography provides an early and objective assessment of the success and failure of axillary regional blockades.},
	URL = {https://doi.org/10.1213/01.ane.0000189556.49429.16},
}




@article{432613,
  title={Chestnut's Obstetric Anesthesia: Principles and Practice E-Book: Expert Consult-Online and Print},
  author={Chestnut, David H. and Wong, Cynthia A. and Tsen, Lawrence C. and Kee, Wee Meng David N. and Beilin, Yaakov and Mhyre, Jill},
  year={2014},
  publisher={Elsevier Health Sciences}
}


@article{12351,
author = {ASGHAR, S. and LUNDSTRØM, L. H. and BJERREGAARD, L. S. and LANGE, K. H. W.},
title = {Ultrasound-guided lateral infraclavicular block evaluated by infrared thermography and distal skin temperature},
journal = {Acta Anaesthesiologica Scandinavica},
volume = {58},
number = {7},
pages = {867-874},
doi = {https://doi.org/10.1111/aas.12351},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/aas.12351},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/aas.12351},
abstract = {Background Brachial plexus blocks cause changes in hand and digit skin temperature. We investigated thermographic patterns after the lateral infraclavicular brachial plexus block. We hypothesised that a successful lateral infraclavicular block could be predicted by increased skin temperature of the 2nd and 5th digits. Methods We performed an ultrasound-guided lateral infraclavicular block in 45 patients undergoing upper limb surgery. The contralateral hand served as control and we obtained infrared thermographic images of both hands before the block and during the following 30 min. We defined areas of interest on the hands and analysed mean skin temperature of each area. Results Forty patients completed the study. Thirty blocks were successful, six were failures and four were partial failures. Four distinct patterns of skin temperature changes were revealed with highly significant changes in temperature, depending on block success. A simultaneous 1 °C ipsilateral increase in skin temperature of the 2nd and 5th digits predicted a successful block with a positive predictive value of 100\%. A 5 °C difference in digit skin temperature compared with the contralateral hand had a positive predictive value of 96\%, and a digit skin temperature ≤ 30 °C 30  min after performing the block had a predictive value of 100\% for a failed block. Conclusions Four different thermographic patterns were found. Simultaneous increases in skin temperature of both the 2nd and 5th digits predicted lateral infraclavicular block success with a positive predictive value of 100\%. Digit skin temperature ≤ 30 °C 30 min after performing the block indicated block failure.},
year = {2014}
}
@article{Lange2011SkinTM,
  title={Skin temperature measured by infrared thermography after specific ultrasound-guided blocking of the musculocutaneous, radial, ulnar, and median nerves in the upper extremity.},
  author={K. H. Lange and T. Jansen and S. Asghar and P. Kristensen and M. Skj{\o}nnemand and P. N{\o}rgaard},
  journal={British journal of anaesthesia},
  year={2011},
  volume={106 6},
  pages={
          887-95
        }
}

@article{https://doi.org/10.1111/anae.15246,
author = {McCombe, K. and Bogod, D.},
title = {Regional anaesthesia: risk, consent and complications},
journal = {Anaesthesia},
volume = {76},
number = {S1},
pages = {18-26},
keywords = {blocks, complications, consent, regional anaesthesia, risk},
doi = {https://doi.org/10.1111/anae.15246},
url = {https://associationofanaesthetists-publications.onlinelibrary.wiley.com/doi/abs/10.1111/anae.15246},
eprint = {https://associationofanaesthetists-publications.onlinelibrary.wiley.com/doi/pdf/10.1111/anae.15246},
abstract = {Summary The risks of regional anaesthesia relate primarily to the technical nature of the procedure, chief among them being neurological. While rare, the direct relationship between nerve damage and the procedure itself means that patients need to be aware of this complication when consent is sought. In order to give valid consent, a patient must be informed. The extent of the information required has been defined by a 2015 legal ruling which established that the standard is the expectation of a reasonable patient, rather than the information deemed consequential by a reasonable doctor. The implications of this for clinicians are profound, and mean that the process of consent must, for example, include alternatives to the proposed treatment. Additionally, patients must have capacity and give their consent without coercion. Effective communication of risk can be challenging. As well as the barriers to comprehension that can result from language, literacy and numeracy, clinicians need to be aware of their own biases, often in favour of a regional anaesthetic approach. Patients also have biases, and doctors must be aware of these in order to best target their provision of information. Careful use of language and employing adjuncts such as information leaflets and visual aids can help to maximise the individual's autonomy. Particular care must be taken in special situations such as where patients have capacity issues or time is limited by the emergency nature of the intervention.},
year = {2021}
}


@incollection{CONHAIM2023112,
title = {17 - Anesthesia for Cardiac Patients During Labor and Delivery},
editor = {John H. Wilson and William T. Schnettler and Adam M. Lubert and Andrea Girnius},
booktitle = {Maternal Cardiac Care},
publisher = {Elsevier},
address = {New Delhi},
pages = {112-119},
year = {2023},
isbn = {978-0-323-82464-4},
doi = {https://doi.org/10.1016/B978-0-323-82464-4.00017-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323824644000177},
author = {Jay Conhaim and Andrea Girnius},
keywords = {epidural analgesia, spinal anesthesia, general anesthesia, neuraxial anesthesia, vaginal delivery, cesarean section},
abstract = {Anesthesiologists are a critical part of the delivery team taking care of pregnant cardiac patients. Providing adequate labor analgesia or safe anesthesia for cesarean section is important to providing optimal patient care and minimizing complications. This chapter reviews relevant cardiac physiologic changes during pregnancy that we must account for, options for labor analgesia, and surgical anesthesia with a discussion of their effects on the cardiovascular system. The chapter also includes brief review of selected cardiac lesions and their anesthetic considerations.}
}

@article{https://doi.org/10.1111/anae.12927,
author = {Hoyle, J. and Yentis, S. M.},
title = {Assessing the height of block for caesarean section over the past three decades: trends from the literature},
journal = {Anaesthesia},
volume = {70},
number = {4},
pages = {421-428},
doi = {https://doi.org/10.1111/anae.12927},
url = {https://associationofanaesthetists-publications.onlinelibrary.wiley.com/doi/abs/10.1111/anae.12927},
eprint = {https://associationofanaesthetists-publications.onlinelibrary.wiley.com/doi/pdf/10.1111/anae.12927},
abstract = {Summary There are multiple methods of assessing the height of block before caesarean section under regional anaesthesia, and surveys of practice suggest considerable variation in practice. So far, little emphasis has been placed on the guidance to be gained from published research literature or textbooks. We therefore set out to investigate the methods of block assessment documented in published articles and textbooks over the past 30 years. We performed two searches of PubMed for randomised clinical trials with caesarean section and either spinal anaesthesia or epidural anaesthesia as major Medical Subject Headings. A total of 284 papers, from 1984 to 2013, were analysed for methods of assessment of sensory and motor block, and the height of block deemed adequate for surgery. We also examined 45 editions of seven anaesthetic textbooks spanning 1950–2014 for recommended methods of assessment and height of block required for caesarean section. Analysis of published papers demonstrated a wide variation in techniques, though there has been a trend towards the increased use of touch, and an increased use of a block height of T5 over the study period. Only 115/284 (40.5\%) papers described the method of assessing motor block, with most of those that did (102/115; 88.7\%) describing it as the ‘Bromage scale’, although only five of these (4.9\%) matched the original description by Bromage. The required height of block recommended by textbooks has risen over the last 30 years to T4, although only four textbooks made any recommendation about the preferred sensory modality. The variation in methods suggested by surveys of practice is reflected in variation in published trials, and there is little consensus or guidance in anaesthetic textbooks.},
year = {2015}
}



@article{CHAE2022104744,
title = {Pain modalities in the body and brain: Current knowledge and future perspectives},
journal = {Neuroscience \& Biobehavioral Reviews},
volume = {139},
pages = {104744},
year = {2022},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2022.104744},
url = {https://www.sciencedirect.com/science/article/pii/S0149763422002330},
author = {Younbyoung Chae and Hi-Joon Park and In-Seon Lee},
keywords = {Pain, Modality specificity, Biomarker, Functional neuroimaging, Precision medicine},
abstract = {Development and validation of pain biomarkers has become a major issue in pain research. Recent advances in multimodal data acquisition have allowed researchers to gather multivariate and multilevel whole-body measurements in patients with pain conditions, and data analysis techniques such as machine learning have led to novel findings in neural biomarkers for pain. Most studies have focused on the development of a biomarker to predict the severity of pain with high precision and high specificity, however, a similar approach to discriminate different modalities of pain is lacking. Identification of more accurate and specific pain biomarkers will require an in-depth understanding of the modality specificity of pain. In this review, we summarize early and recent findings on the modality specificity of pain in the brain, with a focus on distinct neural activity patterns between chronic clinical and acute experimental pain, direct, social, and vicarious pain, and somatic and visceral pain. We also suggest future directions to improve our current strategy of pain management using our knowledge of modality-specific aspects of pain.}
}


@article{10.1093/bmb/ldh035,
    author = {Curatolo, Michele and Petersen-Felix, Steen and Arendt-Nielsen, Lars},
    title = "{Assessment of regional analgesia in clinical practice and research}",
    journal = {British Medical Bulletin},
    volume = {71},
    number = {1},
    pages = {61-76},
    year = {2005},
    month = {01},
    abstract = "{Assessment of pain and sensory function during regional analgesia contributes to a better understanding of the mechanisms underlying the action of drugs and techniques, and provides information on the effectiveness of regional analgesia in daily practice. Sensory tests only partially mimic clinical pain, mainly because they are artificial and reproduce only a part of the complex experience of pain. Therefore information gained by sensory tests should not be uncritically generalized to clinical pain conditions. Studies using experimental pain models are not in competition with studies performed under clinical conditions, but complement them. In order to mirror clinical pain, experimental models ideally stimulate muscles and viscera, induce peripheral and central sensitization, and evoke temporal and spatial summation. These methods are available, but are underused. Test modalities used in clinical practice have limited validity. In recent years almost no research has been performed to develop better test modalities that are suitable for daily practice.}",
    issn = {0007-1420},
    doi = {10.1093/bmb/ldh035},
    url = {https://doi.org/10.1093/bmb/ldh035},
    eprint = {https://academic.oup.com/bmb/article-pdf/71/1/61/25152159/ldh035.pdf},
}


@article{Chen2021,
   abstract = {Medical image segmentation is an essential prerequisite for developing
healthcare systems, especially for disease diagnosis and treatment planning. On
various medical image segmentation tasks, the u-shaped architecture, also known
as U-Net, has become the de-facto standard and achieved tremendous success.
However, due to the intrinsic locality of convolution operations, U-Net
generally demonstrates limitations in explicitly modeling long-range
dependency. Transformers, designed for sequence-to-sequence prediction, have
emerged as alternative architectures with innate global self-attention
mechanisms, but can result in limited localization abilities due to
insufficient low-level details. In this paper, we propose TransUNet, which
merits both Transformers and U-Net, as a strong alternative for medical image
segmentation. On one hand, the Transformer encodes tokenized image patches from
a convolution neural network (CNN) feature map as the input sequence for
extracting global contexts. On the other hand, the decoder upsamples the
encoded features which are then combined with the high-resolution CNN feature
maps to enable precise localization. We argue that Transformers can serve as strong encoders for medical image
segmentation tasks, with the combination of U-Net to enhance finer details by
recovering localized spatial information. TransUNet achieves superior
performances to various competing methods on different medical applications
including multi-organ segmentation and cardiac segmentation. Code and models
are available at https://github.com/Beckschen/TransUNet.},
   author = {Jieneng Chen and Yongyi Lu and Qihang Yu and Xiangde Luo and Ehsan Adeli and Yan Wang and Le Lu and Alan L. Yuille and Yuyin Zhou},
   isbn = {2102.04306v1},
   month = {2},
   title = {TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation},
   url = {https://arxiv.org/abs/2102.04306v1},
   year = {2021},
}


@article{DBLP:journals/corr/abs-2010-11929,
  author    = {Alexey Dosovitskiy and
               Lucas Beyer and
               Alexander Kolesnikov and
               Dirk Weissenborn and
               Xiaohua Zhai and
               Thomas Unterthiner and
               Mostafa Dehghani and
               Matthias Minderer and
               Georg Heigold and
               Sylvain Gelly and
               Jakob Uszkoreit and
               Neil Houlsby},
  title     = {An Image is Worth 16x16 Words: Transformers for Image Recognition
               at Scale},
  journal   = {CoRR},
  volume    = {abs/2010.11929},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.11929},
  eprinttype = {arXiv},
  eprint    = {2010.11929},
  timestamp = {Fri, 20 Nov 2020 14:04:05 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2010-11929.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{Li2021,
   abstract = {Medical image segmentation is important for computer-aided diagnosis. Good
segmentation demands the model to see the big picture and fine details
simultaneously, i.e., to learn image features that incorporate large context
while keep high spatial resolutions. To approach this goal, the most widely
used methods -- U-Net and variants, extract and fuse multi-scale features.
However, the fused features still have small "effective receptive fields" with
a focus on local image cues, limiting their performance. In this work, we
propose Segtran, an alternative segmentation framework based on transformers,
which have unlimited "effective receptive fields" even at high feature
resolutions. The core of Segtran is a novel Squeeze-and-Expansion transformer:
a squeezed attention block regularizes the self attention of transformers, and
an expansion block learns diversified representations. Additionally, we propose
a new positional encoding scheme for transformers, imposing a continuity
inductive bias for images. Experiments were performed on 2D and 3D medical
image segmentation tasks: optic disc/cup segmentation in fundus images
(REFUGE'20 challenge), polyp segmentation in colonoscopy images, and brain
tumor segmentation in MRI scans (BraTS'19 challenge). Compared with
representative existing methods, Segtran consistently achieved the highest
segmentation accuracy, and exhibited good cross-domain generalization
capabilities. The source code of Segtran is released at
https://github.com/askerlee/segtran.},
   author = {Shaohua Li and Xiuchao Sui and Xiangde Luo and Xinxing Xu and Yong Liu and Rick Goh},
   doi = {10.24963/ijcai.2021/112},
   isbn = {9780999241196},
   issn = {10450823},
   journal = {IJCAI International Joint Conference on Artificial Intelligence},
   month = {5},
   pages = {807-815},
   publisher = {International Joint Conferences on Artificial Intelligence},
   title = {Medical Image Segmentation Using Squeeze-and-Expansion Transformers},
   url = {https://arxiv.org/abs/2105.09511v3},
   year = {2021},
}


@misc{kirillov2023segment,
      title={Segment Anything}, 
      author={Alexander Kirillov and Eric Mintun and Nikhila Ravi and Hanzi Mao and Chloe Rolland and Laura Gustafson and Tete Xiao and Spencer Whitehead and Alexander C. Berg and Wan-Yen Lo and Piotr Dollár and Ross Girshick},
      year={2023},
      eprint={2304.02643},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@article{Luo2022,
   author = {Xiangde Luo and Minhao Hu and Tao Song and Guotai Wang and Shaoting Zhang},
   issn = {2640-3498},
   month = {12},
   pages = {820-833},
   publisher = {PMLR},
   title = {Semi-Supervised Medical Image Segmentation via Cross Teaching between CNN and Transformer},
   url = {https://proceedings.mlr.press/v172/luo22b.html},
   year = {2022},
}


@article{Cao2021,
   abstract = {In the past few years, convolutional neural networks (CNNs) have achieved
milestones in medical image analysis. Especially, the deep neural networks
based on U-shaped architecture and skip-connections have been widely applied in
a variety of medical image tasks. However, although CNN has achieved excellent
performance, it cannot learn global and long-range semantic information
interaction well due to the locality of the convolution operation. In this
paper, we propose Swin-Unet, which is an Unet-like pure Transformer for medical
image segmentation. The tokenized image patches are fed into the
Transformer-based U-shaped Encoder-Decoder architecture with skip-connections
for local-global semantic feature learning. Specifically, we use hierarchical
Swin Transformer with shifted windows as the encoder to extract context
features. And a symmetric Swin Transformer-based decoder with patch expanding
layer is designed to perform the up-sampling operation to restore the spatial
resolution of the feature maps. Under the direct down-sampling and up-sampling
of the inputs and outputs by 4x, experiments on multi-organ and cardiac
segmentation tasks demonstrate that the pure Transformer-based U-shaped
Encoder-Decoder network outperforms those methods with full-convolution or the
combination of transformer and convolution. The codes and trained models will
be publicly available at https://github.com/HuCaoFighting/Swin-Unet.},
   author = {Hu Cao and Yueyue Wang and Joy Chen and Dongsheng Jiang and Xiaopeng Zhang and Qi Tian and Manning Wang},
   doi = {10.1007/978-3-031-25066-8_9},
   isbn = {9783031250651},
   issn = {16113349},
   month = {5},
   pages = {205-218},
   title = {Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation},
   url = {https://arxiv.org/abs/2105.05537v1},
   year = {2021},
}


@article{Zhang2021,
   abstract = {Medical image segmentation - the prerequisite of numerous clinical needs -
has been significantly prospered by recent advances in convolutional neural
networks (CNNs). However, it exhibits general limitations on modeling explicit
long-range relation, and existing cures, resorting to building deep encoders
along with aggressive downsampling operations, leads to redundant deepened
networks and loss of localized details. Hence, the segmentation task awaits a
better solution to improve the efficiency of modeling global contexts while
maintaining a strong grasp of low-level details. In this paper, we propose a
novel parallel-in-branch architecture, TransFuse, to address this challenge.
TransFuse combines Transformers and CNNs in a parallel style, where both global
dependency and low-level spatial details can be efficiently captured in a much
shallower manner. Besides, a novel fusion technique - BiFusion module is
created to efficiently fuse the multi-level features from both branches.
Extensive experiments demonstrate that TransFuse achieves the newest
state-of-the-art results on both 2D and 3D medical image sets including polyp,
skin lesion, hip, and prostate segmentation, with significant parameter
decrease and inference speed improvement.},
   author = {Yundong Zhang and Huiye Liu and Qiang Hu},
   doi = {10.1007/978-3-030-87193-2_2},
   isbn = {9783030871925},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   keywords = {Convolutional neural networks,Fusion,Medical image segmentation,Transformers},
   month = {2},
   pages = {14-24},
   publisher = {Springer Science and Business Media Deutschland GmbH},
   title = {TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation},
   volume = {12901 LNCS},
   url = {https://arxiv.org/abs/2102.08005v2},
   year = {2021},
}

@article{Bi2017StackedFC,
  title={Stacked fully convolutional networks with multi-channel learning: application to medical image segmentation},
  author={Lei Bi and Jinman Kim and Ashnil Kumar and Michael J. Fulham and Dagan Feng},
  journal={The Visual Computer},
  year={2017},
  volume={33},
  pages={1061 - 1071}
}

@article{kumar2018automated,
  title={Automated and real-time segmentation of suspicious breast masses using convolutional neural network},
  author={Kumar, Viksit and Webb, Jeremy M and Gregory, Adriana and Denis, Max and Meixner, Duane D and Bayat, Mahdi and Whaley, Dana H and Fatemi, Mostafa and Alizad, Azra},
  journal={PloS one},
  volume={13},
  number={5},
  pages={e0195816},
  year={2018},
  publisher={Public Library of Science San Francisco, CA USA}
}


@misc{chen2017deeplab,
      title={DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs}, 
      author={Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin Murphy and Alan L. Yuille},
      year={2017},
      eprint={1606.00915},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@article{DBLP:journals/corr/abs-1807-10165,
  author    = {Zongwei Zhou and
               Md Mahfuzur Rahman Siddiquee and
               Nima Tajbakhsh and
               Jianming Liang},
  title     = {UNet++: {A} Nested U-Net Architecture for Medical Image Segmentation},
  journal   = {CoRR},
  volume    = {abs/1807.10165},
  year      = {2018},
  url       = {http://arxiv.org/abs/1807.10165},
  eprinttype = {arXiv},
  eprint    = {1807.10165},
  timestamp = {Mon, 13 Aug 2018 16:46:03 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1807-10165.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@misc{badrinarayanan2016segnet,
      title={SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation}, 
      author={Vijay Badrinarayanan and Alex Kendall and Roberto Cipolla},
      year={2016},
      eprint={1511.00561},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{he2018mask,
      title={Mask R-CNN}, 
      author={Kaiming He and Georgia Gkioxari and Piotr Dollár and Ross Girshick},
      year={2018},
      eprint={1703.06870},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@misc{zhao2017pyramid,
      title={Pyramid Scene Parsing Network}, 
      author={Hengshuang Zhao and Jianping Shi and Xiaojuan Qi and Xiaogang Wang and Jiaya Jia},
      year={2017},
      eprint={1612.01105},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{ren2016faster,
      title={Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}, 
      author={Shaoqing Ren and Kaiming He and Ross Girshick and Jian Sun},
      year={2016},
      eprint={1506.01497},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@article{diabeticFoot,
   abstract = {Thermography enables non-invasive, accessible, and easily repeated foot temperature measurements for diabetic patients, promoting early detection and regular monitoring protocols, that limit the incidence of disabling conditions associated with diabetic foot disorders. The establishment of this application into standard diabetic care protocols requires to overcome technical issues, particularly the foot sole segmentation. In this work we implemented and evaluated several segmentation approaches which include conventional and Deep Learning methods. Multimodal images, constituted by registered visual-light, infrared and depth images, were acquired for 37 healthy subjects. The segmentation methods explored were based on both visual-light as well as infrared images, and optimization was achieved using the spatial information provided by the depth images. Furthermore, a ground truth was established from the manual segmentation performed by two independent researchers. Overall, the performance level of all the implemented approaches was satisfactory. Although the best performance, in terms of spatial overlap, accuracy, and precision, was found for the Skin and U-Net approaches optimized by the spatial information. However, the robustness of the U-Net approach is preferred.},
   author = {Natalia Arteaga-Marrero and Abián Hernández and Enrique Villa and Sara González-Pérez and Carlos Luque and Juan Ruiz-Alzola},
   doi = {10.3390/S21030934},
   issn = {1424-8220},
   issue = {3},
   journal = {Sensors 2021, Vol. 21, Page 934},
   keywords = {diabetic foot (D017719),diabetic neuropathy (D003929),segmentation,supervised and unsupervised algorithms,thermography (D013817)},
   month = {1},
   pages = {934},
   pmid = {33573296},
   publisher = {Multidisciplinary Digital Publishing Institute},
   title = {Segmentation Approaches for Diabetic Foot Disorders},
   volume = {21},
   url = {https://www.mdpi.com/1424-8220/21/3/934/htm https://www.mdpi.com/1424-8220/21/3/934},
   year = {2021},
}


@article{10.1145/358669.358692,
author = {Fischler, Martin A. and Bolles, Robert C.},
title = {Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography},
year = {1981},
issue_date = {June 1981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {6},
issn = {0001-0782},
url = {https://doi.org/10.1145/358669.358692},
doi = {10.1145/358669.358692},
abstract = {A new paradigm, Random Sample Consensus (RANSAC), for fitting a model to experimental data is introduced. RANSAC is capable of interpreting/smoothing data containing a significant percentage of gross errors, and is thus ideally suited for applications in automated image analysis where interpretation is based on the data provided by error-prone feature detectors. A major portion of this paper describes the application of RANSAC to the Location Determination Problem (LDP): Given an image depicting a set of landmarks with known locations, determine that point in space from which the image was obtained. In response to a RANSAC requirement, new results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form. These results provide the basis for an automatic system that can solve the LDP under difficult viewing},
journal = {Commun. ACM},
month = {jun},
pages = {381–395},
numpages = {15},
keywords = {camera calibration, scene analysis, location determination, automated cartography, image matching, model fitting}
}



@article{Bouallal2020,
   abstract = {Abnormal plantar foot temperature changes are an early sign of diabetic foot (DF) ulcer, that can be detected using a thermal camera. This communication is composed of two main contributions. The first one concerns the segmentation of plantar foot thermal images. It consists of using the deep learning method U-Net to segment the thermal images. U-Net is trained by combining the two types of images (thermal and color) given by the thermal camera FLIR ONE Pro. Results show that this multimodal approach performs better than the one using only thermal images, especially for difficult cases. The second part is devoted to a transversal clinical study conducted within the Hospital National Dos de Mayo in Lima, Peru. 122 type II diabetic patients without ulcer were recruited. These individuals were classified into three risk groups of developing a foot ulcer. This classification is based on a medical examination: a low-risk group (R0), a medium-risk group (R1) and finally a high-risk group (R2). The study reveals that the average temperature of the plantar foot is 1°C higher in R1 than in R0 (p<0.1). The R1 group patients are characterized by a rapid recovery of their initial temperature after the cold stress test, compared to R0 and R2 (p<0.01). Finally, the mean absolute point-to-point temperature difference between left and right foot is lower in R1 than in R2 (p<0.1). These results demonstrate that thermal camera temperature assessment could help in the diagnosis of diabetic foot.},
   author = {Doha Bouallal and Asma Bougrine and Hassan Douzi and Rachid Harba and Raphael Canals and Luis Vilcahuaman and Hugo Arbanil},
   doi = {10.1109/IWSSIP48289.2020.9145167},
   isbn = {9781728175393},
   issn = {21578702},
   journal = {International Conference on Systems, Signals, and Image Processing},
   keywords = {computer vision,deep learning,diabetic foot,mobile health,segmentation,thermal images},
   month = {7},
   pages = {116-121},
   publisher = {IEEE Computer Society},
   title = {Segmentation of plantar foot thermal images: Application to diabetic foot diagnosis},
   volume = {2020-July},
   year = {2020},
}


@misc{zhou2015learning,
      title={Learning Deep Features for Discriminative Localization}, 
      author={Bolei Zhou and Aditya Khosla and Agata Lapedriza and Aude Oliva and Antonio Torralba},
      year={2015},
      eprint={1512.04150},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{Bougrine2019,
   abstract = {Foot ulceration can be prevented by using thermal information of the plantar foot surface. Indeed, important indicators can be provided with a thermal infrared image. As part of a non-constraining acquisition protocol, these images are freehandedly taken with a smartphone equipped by a dedicated thermal camera. A total of 248 images have been obtained from an acquisition campaign composed of control and pathological subjects. Our aim is the segmentation of these plantar foot thermal images. To that end, we compare three different deep learning methods namely, the Fully Convolutional Networks (FCN), SegNet, U-Net, and the previously proposed prior shape active contour-based method. 80\% of our database serves to train the 3 deep learning networks and 20\% are used for the test. When applied to our data, results show that the SegNet method outperforms the three other methods with a Dice Similarity Coefficient (DSC) equal to 97.26\%. This method also shows efficiency in segmenting both feet simultaneously with a DSC equal to 96.8\% for a smartphone based plantar foot thermal analysis for diabetic patients.},
   author = {Asma Bougrine and Rachid Harba and Raphael Canals and Roger Ledee and Meryem Jabloun},
   doi = {10.23919/EUSIPCO.2019.8902691},
   isbn = {9789082797039},
   issn = {22195491},
   journal = {European Signal Processing Conference},
   keywords = {Deep Learning,Image segmentation,Plantar foot thermal images,Prior shape active contour},
   month = {9},
   publisher = {European Signal Processing Conference, EUSIPCO},
   title = {On the segmentation of plantar foot thermal images with deep learning},
   volume = {2019-September},
   year = {2019},
}

@article{Peng2021,
   abstract = {Transformers are state-of-the-art models for a variety of sequence modeling tasks. At their core is an attention function which models pairwise interactions between the inputs at every timestep. While attention is powerful, it does not scale efficiently to long sequences due to its quadratic time and space complexity in the sequence length. We propose RFA, a linear time and space attention that uses random feature methods to approximate the softmax function, and explore its application in transformers. RFA can be used as a drop-in replacement for conventional softmax attention and offers a straightforward way of learning with recency bias through an optional gating mechanism. Experiments on language model-ing and machine translation demonstrate that RFA achieves similar or better performance compared to strong transformer baselines. In the machine translation experiment, RFA decodes twice as fast as a vanilla transformer. Compared to existing efficient transformer variants, RFA is competitive in terms of both accuracy and efficiency on three long text classification datasets. Our analysis shows that RFA's efficiency gains are especially notable on long sequences, suggesting that RFA will be particularly useful in tasks that require working with large inputs, fast decoding speed, or low memory footprints.},
   author = {Hao Peng and Nikolaos Pappas and Dani Yogatama and Roy Schwartz and Noah A Smith and Lingpeng Kong and Paul G Allen},
   title = {RANDOM FEATURE ATTENTION},
   year = {2021},
}


@article{jimenez2021,
   abstract = {Peripheral nerve blocking (PNB) is a standard procedure to support regional anesthesia. Still, correct localization of the nerve’s structure is needed to avoid adverse effects; thereby, ultrasound images are used as an aid approach. In addition, image-based automatic nerve segmentation from deep learning methods has been proposed to mitigate attenuation and speckle noise ultrasonography issues. Notwithstanding, complex architectures highlight the region of interest lacking suitable data interpretability concerning the learned features from raw instances. Here, a kernel-based deep learning enhancement is introduced for nerve structure segmentation. In a nutshell, a random Fourier features-based approach was utilized to complement three well-known semantic segmentation architectures, e.g., fully convolutional network, U-net, and ResUnet. Moreover, two ultrasound image datasets for PNB were tested. Obtained results show that our kernel-based approach provides a better generalization capability from image segmentation-based assessments on different nerve structures. Further, for data interpretability, a semantic segmentation extension of the GradCam++ for class-activation mapping was used to reveal relevant learned features separating between nerve and background. Thus, our proposal favors both straightforward (shallow) and complex architectures (deeper neural networks).},
   author = {Cristian Alfonso Jimenez-Castaño and Andrés Marino Álvarez-Meza and Oscar David Aguirre-Ospina and David Augusto Cárdenas-Peña and Álvaro Angel Orozco-Gutiérrez},
   doi = {10.3390/S21227741},
   issn = {1424-8220},
   issue = {22},
   journal = {Sensors 2021, Vol. 21, Page 7741},
   keywords = {class activation mapping,deep learning,nerve structure segmentation,random Fourier features,ultrasound images},
   month = {11},
   pages = {7741},
   publisher = {Multidisciplinary Digital Publishing Institute},
   title = {Random Fourier Features-Based Deep Learning Improvement with Class Activation Interpretability for Nerve Structure Segmentation},
   volume = {21},
   url = {https://www.mdpi.com/1424-8220/21/22/7741/htm https://www.mdpi.com/1424-8220/21/22/7741},
   year = {2021},
}

@article{Tancik2020,
   abstract = {We show that passing input points through a simple Fourier feature mapping
enables a multilayer perceptron (MLP) to learn high-frequency functions in
low-dimensional problem domains. These results shed light on recent advances in
computer vision and graphics that achieve state-of-the-art results by using
MLPs to represent complex 3D objects and scenes. Using tools from the neural
tangent kernel (NTK) literature, we show that a standard MLP fails to learn
high frequencies both in theory and in practice. To overcome this spectral
bias, we use a Fourier feature mapping to transform the effective NTK into a
stationary kernel with a tunable bandwidth. We suggest an approach for
selecting problem-specific Fourier features that greatly improves the
performance of MLPs for low-dimensional regression tasks relevant to the
computer vision and graphics communities.},
   author = {Matthew Tancik and Pratul P. Srinivasan and Ben Mildenhall and Sara Fridovich-Keil and Nithin Raghavan and Utkarsh Singhal and Ravi Ramamoorthi and Jonathan T. Barron and Ren Ng},
   month = {6},
   title = {Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains},
   url = {https://arxiv.org/abs/2006.10739v1},
   year = {2020},
}

@article{Xie2019,
   abstract = {Kernel learning methods are among the most effective learning methods and
have been vigorously studied in the past decades. However, when tackling with
complicated tasks, classical kernel methods are not flexible or "rich" enough
to describe the data and hence could not yield satisfactory performance. In
this paper, via Random Fourier Features (RFF), we successfully incorporate the
deep architecture into kernel learning, which significantly boosts the
flexibility and richness of kernel machines while keeps kernels' advantage of
pairwise handling small data. With RFF, we could establish a deep structure and
make every kernel in RFF layers could be trained end-to-end. Since RFF with
different distributions could represent different kernels, our model has the
capability of finding suitable kernels for each layer, which is much more
flexible than traditional kernel-based methods where the kernel is
pre-selected. This fact also helps yield a more sophisticated kernel cascade
connection in the architecture. On small datasets (less than 1000 samples), for
which deep learning is generally not suitable due to overfitting, our method
achieves superior performance compared to advanced kernel methods. On
large-scale datasets, including non-image and image classification tasks, our
method also has competitive performance.},
   author = {Jiaxuan Xie and Fanghui Liu and Kaijie Wang and Xiaolin Huang},
   doi = {10.48550/arxiv.1910.02660},
   month = {10},
   title = {Deep Kernel Learning via Random Fourier Features},
   url = {https://arxiv.org/abs/1910.02660v1},
   year = {2019},
}



@article{Morrow2017,
   abstract = {We present a simple and efficient method for prediction of transcription factor binding sites from DNA sequence. Our method computes a random approximation of a convolutional kernel feature map from DNA sequence and then learns a linear model from the approximated feature map. Our method outperforms state-of-the-art deep learning methods on five out of six test datasets from the ENCODE consortium, while training in less than one eighth the time.},
   author = {Alyssa Morrow and Vaishaal Shankar and Devin Petersohn and Anthony Joseph and Benjamin Recht and Nir Yosef},
   month = {5},
   title = {Convolutional Kitchen Sinks for Transcription Factor Binding Site Prediction},
   url = {http://arxiv.org/abs/1706.00125},
   year = {2017},
}


@article{Mairal2014,
   abstract = {An important goal in visual recognition is to devise image representations that are invariant to particular transformations. In this paper, we address this goal with a new type of convolutional neural network (CNN) whose invariance is encoded by a reproducing kernel. Unlike traditional approaches where neural networks are learned either to represent data or for solving a classification task, our network learns to approximate the kernel feature map on training data. Such an approach enjoys several benefits over classical ones. First, by teaching CNNs to be invariant, we obtain simple network architectures that achieve a similar accuracy to more complex ones, while being easy to train and robust to overfitting. Second, we bridge a gap between the neural network literature and kernels, which are natural tools to model invariance. We evaluate our methodology on visual recognition tasks where CNNs have proven to perform well, e.g., digit recognition with the MNIST dataset, and the more challenging CIFAR-10 and STL-10 datasets, where our accuracy is competitive with the state of the art.},
   author = {Julien Mairal and Piotr Koniusz and Zaid Harchaoui and Cordelia Schmid},
   month = {6},
   title = {Convolutional Kernel Networks},
   url = {http://arxiv.org/abs/1406.3332},
   year = {2014},
}



@article{Reza2018,
   abstract = {Convolutional Kernel Networks (CKNs) are efficient multilayer kernel machines, which are constructed by approximating a convolution kernel with a mapping based on Gaussian functions. In this paper, we introduce a new approximation of the same convolution kernel based on a convex combination of cosine kernels. CKNs are structurally similar to Convolutional Neural Networks (CNNs), but the convolution operation in CKNs is based on the Euclidean distance, which is not common in convolutional networks. We show that the CKN model obtained by the proposed approximation leads to the ordinary convolution operation, which is based on the inner product. From this point of view, the proposed model is a step forward towards bridging the gap between kernel methods and deep learning. In this paper, we use two methods for learning filters of the proposed CKN: Random Fourier Features, which is a randomized data-independent method for approximating shift-invariant kernels, and a novel method based on the minimization of the sum of squared errors of approximating shift-invariant kernels. Although the RFF method is much faster than ordinary CKN, it requires a high number of random features in order to obtain an acceptable accuracy. To overcome this problem, we proposed the second method, in which the filters are learned in a data-dependent fashion. We evaluate the proposed model on visual recognition datasets MNIST, CIFAR-10, C-Cube, and FERET. Our experiments show that the proposed model surpasses ordinary CKNs in terms of accuracy. Specifically, on CIFAR-10, the accuracy of the proposed method is 1.7\% higher than ordinary CKN.},
   author = {Mohammad Reza Mohammadnia-Qaraei and Reza Monsefi and Kamaledin Ghiasi-Shirazi},
   doi = {10.1016/j.patrec.2018.09.016},
   issn = {01678655},
   journal = {Pattern Recognition Letters},
   keywords = {Convolutional kernel networks (CKN),Convolutional neural networks (CNN),Kernel approximation,Random fourier features (RFF),Sum of squared errors},
   month = {12},
   pages = {127-134},
   publisher = {Elsevier B.V.},
   title = {Convolutional kernel networks based on a convex combination of cosine kernels},
   volume = {116},
   year = {2018},
}

@article{Selvaraju2016,
   abstract = {We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo at http://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E.},
   author = {Ramprasaath R. Selvaraju and Michael Cogswell and Abhishek Das and Ramakrishna Vedantam and Devi Parikh and Dhruv Batra},
   doi = {10.1007/s11263-019-01228-7},
   issue = {2},
   journal = {International Journal of Computer Vision},
   keywords = {Explanations,Grad-CAM,Interpretability,Transparency,Visual explanations,Visualizations},
   month = {10},
   pages = {336-359},
   publisher = {Springer},
   title = {Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization},
   volume = {128},
   url = {http://arxiv.org/abs/1610.02391 http://dx.doi.org/10.1007/s11263-019-01228-7},
   year = {2016},
}

@article{Wang2019,
   abstract = {Recently, increasing attention has been drawn to the internal mechanisms of convolutional neural networks, and the reason why the network makes specific decisions. In this paper, we develop a novel post-hoc visual explanation method called Score-CAM based on class activation mapping. Unlike previous class activation mapping based approaches, Score-CAM gets rid of the dependence on gradients by obtaining the weight of each activation map through its forward passing score on target class, the final result is obtained by a linear combination of weights and activation maps. We demonstrate that Score-CAM achieves better visual performance and fairness for interpreting the decision making process. Our approach outperforms previous methods on both recognition and localization tasks, it also passes the sanity check. We also indicate its application as debugging tools. Official code has been released.},
   author = {Haofan Wang and Zifan Wang and Mengnan Du and Fan Yang and Zijian Zhang and Sirui Ding and Piotr Mardziel and Xia Hu},
   month = {10},
   title = {Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks},
   url = {http://arxiv.org/abs/1910.01279},
   year = {2019},
}

@misc{luo2017understanding,
      title={Understanding the Effective Receptive Field in Deep Convolutional Neural Networks}, 
      author={Wenjie Luo and Yujia Li and Raquel Urtasun and Richard Zemel},
      year={2017},
      eprint={1701.04128},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@ARTICLE{10014654,
  author={Wilson, A. N. and Gupta, Khushi Anil and Koduru, Balu Harshavardan and Kumar, Abhinav and Jha, Ajit and Cenkeramaddi, Linga Reddy},
  journal={IEEE Sensors Journal}, 
  title={Recent Advances in Thermal Imaging and its Applications Using Machine Learning: A Review}, 
  year={2023},
  volume={23},
  number={4},
  pages={3395-3407},
  doi={10.1109/JSEN.2023.3234335}}


@article{Zhou2020,
   abstract = {The state-of-the-art models for medical image segmentation are variants of U-Net and fully convolutional networks (FCN). Despite their success, these models have two limitations: (1) their optimal depth is apriori unknown, requiring extensive architecture search or inefficient ensemble of models of varying depths; and (2) their skip connections impose an unnecessarily restrictive fusion scheme, forcing aggregation only at the same-scale feature maps of the encoder and decoder sub-networks. To overcome these two limitations, we propose UNet++, a new neural architecture for semantic and instance segmentation, by (1) alleviating the unknown network depth with an efficient ensemble of U-Nets of varying depths, which partially share an encoder and co-learn simultaneously using deep supervision; (2) redesigning skip connections to aggregate features of varying semantic scales at the decoder sub-networks, leading to a highly flexible feature fusion scheme; and (3) devising a pruning scheme to accelerate the inference speed of UNet++. We have evaluated UNet++ using six different medical image segmentation datasets, covering multiple imaging modalities such as computed tomography (CT), magnetic resonance imaging (MRI), and electron microscopy (EM), and demonstrating that (1) UNet++ consistently outperforms the baseline models for the task of semantic segmentation across different datasets and backbone architectures; (2) UNet++ enhances segmentation quality of varying-size objects - an improvement over the fixed-depth U-Net; (3) Mask RCNN++ (Mask R-CNN with UNet++ design) outperforms the original Mask R-CNN for the task of instance segmentation; and (4) pruned UNet++ models achieve significant speedup while showing only modest performance degradation. Our implementation and pre-trained models are available at https://github.com/MrGiovanni/UNetPlusPlus.},
   author = {Zongwei Zhou and Md Mahfuzur Rahman Siddiquee and Nima Tajbakhsh and Jianming Liang},
   doi = {10.1109/TMI.2019.2959609},
   issn = {1558254X},
   issue = {6},
   journal = {IEEE Transactions on Medical Imaging},
   keywords = {Neuronal structure segmentation,brain tumor segmentation,cell segmentation,deep supervision,instance segmentation,liver segmentation,lung nodule segmentation,medical image segmentation,model pruning,nuclei segmentation,semantic segmentation},
   month = {6},
   pages = {1856-1867},
   pmid = {31841402},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {UNet++: Redesigning Skip Connections to Exploit Multiscale Features in Image Segmentation},
   volume = {39},
   year = {2020},
}


@article{Chattopadhyay2017,
   abstract = {Over the last decade, Convolutional Neural Network (CNN) models have been highly successful in solving complex vision problems. However, these deep models are perceived as "black box" methods considering the lack of understanding of their internal functioning. There has been a significant recent interest in developing explainable deep learning models, and this paper is an effort in this direction. Building on a recently proposed method called Grad-CAM, we propose a generalized method called Grad-CAM++ that can provide better visual explanations of CNN model predictions, in terms of better object localization as well as explaining occurrences of multiple object instances in a single image, when compared to state-of-the-art. We provide a mathematical derivation for the proposed method, which uses a weighted combination of the positive partial derivatives of the last convolutional layer feature maps with respect to a specific class score as weights to generate a visual explanation for the corresponding class label. Our extensive experiments and evaluations, both subjective and objective, on standard datasets showed that Grad-CAM++ provides promising human-interpretable visual explanations for a given CNN architecture across multiple tasks including classification, image caption generation and 3D action recognition; as well as in new settings such as knowledge distillation.},
   author = {Aditya Chattopadhyay and Anirban Sarkar and Prantik Howlader and Vineeth N Balasubramanian},
   doi = {10.1109/WACV.2018.00097},
   journal = {Proceedings - 2018 IEEE Winter Conference on Applications of Computer Vision, WACV 2018},
   month = {10},
   pages = {839-847},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks},
   volume = {2018-January},
   url = {http://arxiv.org/abs/1710.11063 http://dx.doi.org/10.1109/WACV.2018.00097},
   year = {2017},
}



